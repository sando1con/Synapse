import os
import json
import numpy as np
from collections import defaultdict

# ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨ íŒ¨í‚¤ì§€
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import TfidfVectorizer

# í…ìŠ¤íŠ¸ ê´€ë ¨ íŒ¨í‚¤ì§€
from konlpy.tag import Okt
from sentence_transformers import SentenceTransformer

# ì‹œê°í™” íŒ¨í‚¤ì§€
import matplotlib.pyplot as plt

# Okt ê°ì²´ ìƒì„± (í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•´)
okt = Okt()

# =================== ì„ë² ë”© ê´€ë ¨ í•¨ìˆ˜ ===================

def encode_documents(texts, model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS"):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ SentenceTransformerë¥¼ ì´ìš©í•˜ì—¬ ë²¡í„°ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
    
    ì¸ì:
        texts (list): ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        model_name (str): ì‚¬ìš©í•  pre-trained ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’ ì œê³µ)
    ë°˜í™˜:
        np.array: ê° í…ìŠ¤íŠ¸ì˜ ë²¡í„° í‘œí˜„ (ì •ê·œí™” í¬í•¨)
    """
    model = SentenceTransformer(model_name)
    vectors = model.encode(texts, normalize_embeddings=True)
    return vectors

# =================== ì°¨ì› ì¶•ì†Œ ë° ì •ê·œí™” í•¨ìˆ˜ ===================

def apply_pca_and_normalize(vectors, n_components=2, random_state=42):
    """
    ê³ ì°¨ì› ë²¡í„°ì— PCAë¥¼ ì ìš©í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•œ í›„, í‘œì¤€í™”í•©ë‹ˆë‹¤.
    
    ì¸ì:
        vectors (np.array): ì…ë ¥ ë²¡í„°
        n_components (int): ì¶•ì†Œí•  ì°¨ì› ìˆ˜ (ê¸°ë³¸ 2)
        random_state (int): ëœë¤ ì‹œë“œ
    ë°˜í™˜:
        tuple: (ì •ê·œí™”ëœ ë²¡í„°, PCA ëª¨ë¸, Scaler ëª¨ë¸)
    """
    pca = PCA(n_components=n_components, random_state=random_state)
    reduced = pca.fit_transform(vectors)
    scaler = StandardScaler()
    normalized = scaler.fit_transform(reduced)
    return normalized, pca, scaler

# =================== í´ëŸ¬ìŠ¤í„°ë§ í•¨ìˆ˜ ===================

def find_best_k(vectors, k_min=2, max_ratio=0.15, random_state=42):
    """
    Silhouette ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜(K)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    
    ì¸ì:
        vectors (np.array): í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ìƒ ë²¡í„°
        k_min (int): ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìˆ˜
        max_ratio (float): ì „ì²´ ë¬¸ì„œ ìˆ˜ ëŒ€ë¹„ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¹„ìœ¨
        random_state (int): ëœë¤ ì‹œë“œ
        
    ë°˜í™˜:
        int: ì„ íƒëœ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ (best K)
    """
    print("ğŸ“Œ [DEBUG] find_best_k() í˜¸ì¶œë¨!")
    n_docs = len(vectors)
    k_max = max(k_min + 1, int(n_docs * max_ratio))
    k_max = min(k_max, n_docs - 1)

    scores = []
    for k in range(k_min, k_max + 1):
        kmeans = KMeans(n_clusters=k, random_state=random_state)
        labels = kmeans.fit_predict(vectors)
        score = silhouette_score(vectors, labels)
        print(f"K={k} â†’ Silhouette Score: {score:.3f}")
        scores.append((k, score))

    best_score = max(score for _, score in scores)

    # best_scoreë³´ë‹¤ 0.05 ì´ìƒ ë–¨ì–´ì§€ì§€ ì•ŠëŠ” í›„ë³´ ì¤‘ ê°€ì¥ í° K ì„ íƒ
    candidates = [k for k, s in scores if s >= best_score - 0.05]
    best_k = max(candidates)
    
    print(f"\n[ì„ íƒëœ í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k} (ìœ ì‚¬ ì ìˆ˜ ì¤‘ ìµœëŒ€ K)]\n")
    return best_k

def cluster_documents_kmeans(vectors, auto_k=True, default_k=5, random_state=42):
    """
    KMeans í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
      - auto_kê°€ Trueë©´ find_best_k()ë¥¼ í†µí•´ ìµœì ì˜ Kë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
      - ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ default_k ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    ì¸ì:
        vectors (np.array): í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ìƒ ë²¡í„°
        auto_k (bool): ìë™ìœ¼ë¡œ Kë¥¼ ê²°ì •í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ True)
        default_k (int): ìë™ ê²°ì • ë¯¸ì‚¬ìš© ì‹œ í´ëŸ¬ìŠ¤í„° ìˆ˜
        random_state (int): ëœë¤ ì‹œë“œ
    ë°˜í™˜:
        tuple: (KMeans ëª¨ë¸, ê° ë²¡í„°ì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨, ì„ íƒëœ í´ëŸ¬ìŠ¤í„° ìˆ˜)
    """
    if auto_k:
        best_k = find_best_k(vectors, k_min=3, max_ratio=0.15, random_state=random_state)
    else:
        best_k = default_k

    kmeans = KMeans(n_clusters=best_k, random_state=random_state)
    labels = kmeans.fit_predict(vectors)
    return kmeans, labels, best_k

# =================== í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ===================

def extract_nouns_from_texts(texts):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì„œì— ëŒ€í•´ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ì¸ì:
        texts (list): ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    ë°˜í™˜:
        list: ê° ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ëª…ì‚¬ë“¤ì˜ ë¬¸ìì—´ ëª©ë¡
    """
    return [" ".join(okt.nouns(text)) for text in texts]

def extract_representative_keywords(texts, labels, top_n=5):
    """
    í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ëŒ€í‘œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
      - ê° í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë¬¸ì„œë“¤ì˜ ëª…ì‚¬ ì¶”ì¶œ í›„ TF-IDF ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ top_n í‚¤ì›Œë“œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    
    ì¸ì:
        texts (list): ì „ì²˜ë¦¬ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        labels (list or np.array): ê° ë¬¸ì„œì— í• ë‹¹ëœ í´ëŸ¬ìŠ¤í„° ë¼ë²¨
        top_n (int): ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜ (ê¸°ë³¸ 5)
    ë°˜í™˜:
        dict: í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ í‚¤ë¡œ, ëŒ€í‘œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ ê°–ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    cluster_texts = defaultdict(list)
    for i, label in enumerate(labels):
        cluster_texts[label].append(texts[i])
    
    cluster_keywords = {}
    for label, docs in cluster_texts.items():
        noun_docs = extract_nouns_from_texts(docs)
        vectorizer = TfidfVectorizer(max_features=top_n)
        tfidf_matrix = vectorizer.fit_transform(noun_docs)
        keywords = vectorizer.get_feature_names_out()
        cluster_keywords[label] = list(keywords)
    return cluster_keywords

# =================== ì‹œê°í™” í•¨ìˆ˜ ===================

def visualize_clusters(reduced_vectors, labels, file_paths):
    """
    2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œëœ ë²¡í„°ì™€ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ ì´ìš©í•˜ì—¬ ì‚°ì ë„ í˜•íƒœë¡œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    ì¸ì:
        reduced_vectors (np.array): 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œë˜ê³  ì •ê·œí™”ëœ ë²¡í„° (n_documents x 2)
        labels (list or np.array): ê° ë¬¸ì„œì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨
        file_paths (list): ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (íŒŒì¼ ì´ë¦„ í‘œì‹œìš©)
    """
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=labels, cmap='tab10', alpha=0.7)
    for i, file in enumerate(file_paths):
        plt.text(reduced_vectors[i, 0], reduced_vectors[i, 1], os.path.basename(file), fontsize=8, ha='right')
    plt.xlabel("PCA1")
    plt.ylabel("PCA2")
    plt.title("KMeans Clustering with Sentence-BERT")
    plt.colorbar(scatter, label="Cluster")
    plt.tight_layout()
    plt.show(block=False)